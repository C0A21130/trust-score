# 基本的実装方法

グラフ分析やGraph Neural Network(GNN)に存在する複数のモデルの基礎的な活用方法について紹介する

- 01graph.ipynb
    - NetworkXの基本的な利用方法について解説している
    - ネットワークの可視化, 次数等
- 02centrality.ipynb
    - NetworkXを活用した中心性分析について解説している
    - 次数中心性, 近傍中心性, 媒介中心性, 固有ベクトル中心性, PageRank
- 03search.ipynb
    - ネットワークにおける経路探索について解説している
    - 経路探索, ダイクストラ法
- 04group.ipynb
    - ネットワークにの分割について解説している
    - ネットワーク分析とネットワークからのコミュニティ抽出
- 05model.ipynb
    - ネットワークモデルを活用した生成手法について解説している
    - ランダムモデル, コンフィグレーションモデル等
- gcn.ipynb
    - Graph Convolution Network(GCN)について解説している
    - リンク予測
- graphrnn.ipynb
    - Graph Recoren Neural Network(GraphRNN)について解説している
    - 順序に基づいたネットワーク生成
- node2vec.ipynb
    - Node2Vecについて解説している
    - スキップグラムに基づいたノードの埋め込み
- vgae.ipynb
    - Variational Graph AutoEncoder(VGAE)について解説している
    - ネットワーク生成

## Node2Vec

特定のタスクに依存しない汎用的な特徴ベクトルを埋め込む手法
- ネットワークの豊かなトポロジー情報を保持しながら埋め込みが可能である
- 表現学習：データから直接的に特徴表現を自動で学習する

**Node2Vecの構造**

スキップグラムを応用したモデル
- スキップグラム
    - 文における次の単語を予測する手法
    - Word2Vecなどに応用されている
- Node2Vecにおいては生成されたウォーク(頂点の列)を学習させることで次の頂点を予測する
    - Node2Vecでは2つのパラメータ $p$ , $q$ を導入してウォークを生成する
- BPSやDPSに基づ探索によりサンプリングすることによりネットワークのトポロジーを捉えることを可能にする
    - BFS：幅優先探索, DFS：深さ優先探索
    - $p$ を小さくすると直前のノードに戻りやすく、深さ優先探索（DFS）に近い経路が得られる
    - $q$ を小さくすると出発点から遠ざかる遷移をしやすく、幅優先探索（BFS）に近い経路が得られる

**目的関数**
- ノードの埋め込みベクトルが、ランダムウォークで得られた近傍ノードを高い確率で予測できるようにする

## Graph Convolution Network(GCN)

メッセージ伝達に伴い周囲の頂点情報を集約することで頂点情報を畳み込むグラフニューラルネットワーク
- この実装ではリンク予測のタスクを実行する
- 2値分類(エッジが接続される場合とエッジが接続されない場合)を応用することでリンク予測をする
    - 正のエッジ: エッジが接続されるい確率
    - 負のエッジ: エッジが接続されない確率
- [gcn.ipynb](/trust-engine/basic/gcn.ipynb)

**CNNとの比較**
画像における畳み込みニューラルネットワーク(CNN)では周囲の画素情報を集約するがGCNでは接続されているノードの情報を集約する

**GCNの構造**

``` python
def __init__(self, in_channels, out_channels, dropout=0.5):
    super(GCN, self).__init__()  # 親クラス（nn.Module）の初期化
    self.conv1 = GCNConv(in_channels, 16)    # 1層目：入力層→隠れ層(16次元)
    self.conv2 = GCNConv(16, out_channels)   # 2層目：隠れ層(16)→出力層
    self.dropout = dropout                    # ドロップアウト率を保存
```

- in_channels: 入力特徴量の次元数
- out_channels: 出力特徴量の次元数
- dropout: 過学習防止のためのドロップアウト率（デフォルト0.5

``` python
def forward(self, x, edge_index):
    x = self.conv1(x, edge_index) # 1層目のGCN畳み込み
    x = F.relu(x)                 # ReLU活性化関数
    x = F.dropout(x, self.dropout, training=self.training)  # ドロップアウト
    x = self.conv2(x, edge_index) # 2層目のGCN畳み込み
    return x
```

- x: ノードの特徴量行列 [ノード数, 特徴量次元]
- edge_index: グラフの接続情報 [2, エッジ数]

**目的関数**
- バイナリークロスエントロピー: 二値分類問題における損失関数
- AUC: 主に機械学習の分類モデルの性能評価に使用される指標

## Vartiational Graph Auto-Encoder(VGAE)

VAEの概念をグラフデータに適用し、グラフの構造とノードの特徴を学習しネットワーク生成を行うモデルである

- 変分オートエンコーダ（VAE）の確率的潜在変数モデルの思想を、グラフ構造データへと拡張したフレームワークである
- VGAEは、GNNをベースにした教師なし学習モデルで、ネットワークの構造を潜在空間に圧縮し、再構成することでネットワーク生成を行う。
- [vgae.ipynb](/trust-engine/basic/vgae.ipynb)

**VGAEの構造**
VGAEはグラフエンコーダーとグラフデコーダーに2つから構成される

- グラフエンコーダー $p_\phi(Z|X)$ は取引ネットワーク(隣接行列 $X$ , 頂点特徴量 $A$ )を潜在変数 $Z$ に埋め込む
    - 潜在変数$Z$は平均値 $\mu$ と標準偏差 $\sigma$ による正規分布に従っている
    - 潜在変数 $Z$ はGCNの埋め込みによって求められる
    ```math
    Z = \mathcal{N}(\mu, \sigma^2),
    \quad \mu = \text{GCN}_\mu(X, A),
    \quad \sigma = \text{GCN}_\sigma(X, A)
    ```
- グラフデコーダー $p_\theta(X|Z)$ はグラフエンコーダーによって埋め込まれた潜在変数 $Z$ を基にネットワークを生成する
    - $Z$ の積によりユーザー間の類似度を求める
    - 類似度を活性化関数 $\sigma$ を利用して接続する確率に変換する
    ```math
    X = \sigma(Z_i^\top Z_j)
    ```

**目的関数**
- 再構成誤差 
    - 入力されたグラフの隣接行列と類似したグラフの隣接行列が生成されていることを評価する
    ```math
    E = \frac{1}{N} \sum_{n} \sum_{m} ({-x_{nm}} \log y_{nm} - (1 - x_{nm}) \log (1 - y_{nm}))
    ```
- KL(Kullback-Leibler)ダイバージェンス
    - 潜在変数が正規分布に従ってランダムに生成されていることを評価する
    ```math
    D_{KL}(p||q) = \int_{\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx
    ```

## Graph Recurrent Neural Network(GraphRNN)
GraphRNNは、グラフ生成の課題、特に順列不変性と複雑な依存関係の問題に対して、自己回帰的なアプローチを導入した画期的なモデルです 。その核心は、グラフという非シーケンシャルなオブジェクトを、一連のより単純な条件付き確率ステップに分解することにあります。

- GraphRNNの基本的な発想は、グラフ生成という複雑な非構造的問題を、構造化された逐次的な問題に変換することです。具体的には、あるノード順序$\pi$が与えられたグラフGを、隣接ベクトルのシーケンス$S^{\pi} = (S^{\pi}_1,..., S^{\pi}_n)$にマッピングします。
- GraphRNNの最も重要な革新の一つは、順列不変性の問題に対処し、スケーラビリティを確保するために、幅優先探索（BFS）に基づく正準的なノード順序付けを導入した点です 。
- [graphrnn.ipynb](/trust-engine/basic/graphrnn.ipynb)

**GraphRNNの構造**
- グラフレベルRNN (Graph-Level RNN): これは「マスター」RNNとして機能します。各ステップiで、前のステップの隠れ状態と、直前に生成された隣接ベクトル$S^{\pi}_{i-1}$を入力として受け取ります。その役割は、これまでに生成されたグラフ全体の「状態」を要約した表現$h_i$を維持し、次に新しいノードを追加するか、あるいはグラフ生成を終了するかを決定することです。この$h_i$は、単なるシーケンスの履歴ではなく、それまでに構築されたグラフ$G_{<i}$の構造的特徴を学習した埋め込みベクトル、すなわち「現在のグラフの状態」を表現するものと解釈できます。
- エッジレベルRNN (Edge-Level RNN): これは「ワーカー」RNNとして機能します。グラフレベルRNNの隠れ状態h_iによって初期化され、新しく追加されたノード$\pi(v_i)$が、既存のどのノードと接続するかを決定します。つまり、新しい隣接ベクトル$S^{\pi}_i$を、1つの接続ずつ自己回帰的に生成します 。このエッジレベルのRNNは、新しいエッジ間の複雑な依存関係（例：トライアングルを形成する傾向など）を捉えることができます。

**目的関数**
- バイナリークロスエントロピー: 二値分類問題における損失関数
- AUC: 主に機械学習の分類モデルの性能評価に使用される指標
