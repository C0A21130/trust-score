# Graph Neural Network(GNN)の基礎

GNNに存在する複数のモデルの基礎的な活用方法について紹介する

## Graph Convolution Network(GCN)

メッセージ伝達に伴い周囲の頂点情報を集約することで頂点情報を畳み込むグラフニューラルネットワーク

**CNNとの比較**
画像における畳み込みニューラルネットワーク(CNN)では周囲の画素情報を集約するがGCNでは接続されているノードの情報を集約する

### GCNの実装

**GCNの構造**

``` python
def __init__(self, in_channels, out_channels, dropout=0.5):
    super(GCN, self).__init__()  # 親クラス（nn.Module）の初期化
    self.conv1 = GCNConv(in_channels, 16)    # 1層目：入力層→隠れ層(16次元)
    self.conv2 = GCNConv(16, out_channels)   # 2層目：隠れ層(16)→出力層
    self.dropout = dropout                    # ドロップアウト率を保存
```

- in_channels: 入力特徴量の次元数
- out_channels: 出力特徴量の次元数
- dropout: 過学習防止のためのドロップアウト率（デフォルト0.5

``` python
def forward(self, x, edge_index):
    x = self.conv1(x, edge_index) # 1層目のGCN畳み込み
    x = F.relu(x)                 # ReLU活性化関数
    x = F.dropout(x, self.dropout, training=self.training)  # ドロップアウト
    x = self.conv2(x, edge_index) # 2層目のGCN畳み込み
    return x
```

- x: ノードの特徴量行列 [ノード数, 特徴量次元]
- edge_index: グラフの接続情報 [2, エッジ数]

**目的関数**
- バイナリークロスエントロピー: 二値分類問題における損失関数
- AUC: 主に機械学習の分類モデルの性能評価に使用される指標

## Vartiational Graph Auto-Encoder(VGAE)

VAEの概念をグラフデータに適用し、グラフの構造とノードの特徴を学習しネットワーク生成を行うモデルである
- 変分オートエンコーダ（VAE）の確率的潜在変数モデルの思想を、グラフ構造データへと拡張したフレームワークである
- VGAEは、グラフの隣接行列やノードの特徴量を用いて、グラフ構造を潜在空間にエンコードする

### VGAEの実装

**VGAEの構造**
- グラフエンコーダ：入力されたグラフ構造とノード特徴を低次元の潜在変数に変換する(GCNなどを用いる)
- グラフデコーダー：入力された潜在変数から、元のグラフの隣接行列を再構築(MLPなどを用いる)

**目的関数**
- 再構成誤差: 入力されたグラフの隣接行列と類似したグラフの隣接行列が生成されていることを評価する
- KLダイバージェンス: 潜在変数が正規分布に従ってランダムに生成されていることを評価する

## Recurent Recurrent Neural Network(GraphRNN)
GraphRNNは、グラフ生成の課題、特に順列不変性と複雑な依存関係の問題に対して、自己回帰的なアプローチを導入した画期的なモデルです 。その核心は、グラフという非シーケンシャルなオブジェクトを、一連のより単純な条件付き確率ステップに分解することにあります。

- GraphRNNの基本的な発想は、グラフ生成という複雑な非構造的問題を、構造化された逐次的な問題に変換することです。具体的には、あるノード順序$\pi$が与えられたグラフGを、隣接ベクトルのシーケンス$S^{\pi} = (S^{\pi}_1,..., S^{\pi}_n)$にマッピングします。
- GraphRNNの最も重要な革新の一つは、順列不変性の問題に対処し、スケーラビリティを確保するために、幅優先探索（BFS）に基づく正準的なノード順序付けを導入した点です 。

### GraphRNNの実装

**GraphRNNの構造**
- グラフレベルRNN (Graph-Level RNN): これは「マスター」RNNとして機能します。各ステップiで、前のステップの隠れ状態と、直前に生成された隣接ベクトル$S^{\pi}_{i-1}$を入力として受け取ります。その役割は、これまでに生成されたグラフ全体の「状態」を要約した表現$h_i$を維持し、次に新しいノードを追加するか、あるいはグラフ生成を終了するかを決定することです。この$h_i$は、単なるシーケンスの履歴ではなく、それまでに構築されたグラフ$G_{<i}$の構造的特徴を学習した埋め込みベクトル、すなわち「現在のグラフの状態」を表現するものと解釈できます。
- エッジレベルRNN (Edge-Level RNN): これは「ワーカー」RNNとして機能します。グラフレベルRNNの隠れ状態h_iによって初期化され、新しく追加されたノード$\pi(v_i)$が、既存のどのノードと接続するかを決定します。つまり、新しい隣接ベクトル$S^{\pi}_i$を、1つの接続ずつ自己回帰的に生成します 。このエッジレベルのRNNは、新しいエッジ間の複雑な依存関係（例：トライアングルを形成する傾向など）を捉えることができます。

**目的関数**
- バイナリークロスエントロピー: 二値分類問題における損失関数
- AUC: 主に機械学習の分類モデルの性能評価に使用される指標
